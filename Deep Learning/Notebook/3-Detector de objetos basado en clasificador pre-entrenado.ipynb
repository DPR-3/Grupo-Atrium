{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'imagenet_classes.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     13\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Redimensionar a 224x224\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),  \u001b[38;5;66;03m# Convertir a tensor\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),  \u001b[38;5;66;03m# Normalizar\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Cargar las etiquetas de ImageNet\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet_classes.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_frame\u001b[39m(frame):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imagenet_classes.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Cargar el modelo pre-entrenado ResNet50\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()  # Poner el modelo en modo evaluación\n",
    "\n",
    "# Preprocesamiento de imágenes para el modelo\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionar a 224x224\n",
    "    transforms.ToTensor(),  # Convertir a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalizar\n",
    "])\n",
    "\n",
    "# Cargar las etiquetas de ImageNet\n",
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Preprocesa un fotograma para que sea compatible con el modelo ResNet50.\n",
    "    \"\"\"\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convertir de BGR a RGB\n",
    "    frame = Image.fromarray(frame)  # Convertir a PIL Image\n",
    "    frame = preprocess(frame)  # Aplicar el preprocesamiento\n",
    "    frame = frame.unsqueeze(0)  # Añadir dimensión del batch\n",
    "    return frame\n",
    "\n",
    "def detect_objects_in_frame(frame):\n",
    "    \"\"\"\n",
    "    Detecta objetos en un fotograma utilizando el modelo ResNet50.\n",
    "    Devuelve la clase más probable y la confianza.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Desactivar el cálculo de gradientes\n",
    "        outputs = model(frame)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    confidence = torch.nn.functional.softmax(outputs, dim=1)[0] * 100\n",
    "    class_name = labels[predicted.item()]\n",
    "    confidence = confidence[predicted.item()].item()\n",
    "    return class_name, confidence\n",
    "\n",
    "def process_video_frames(frames_dir):\n",
    "    \"\"\"\n",
    "    Procesa todos los fotogramas de un vídeo y devuelve un DataFrame con los resultados.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Listar todos los fotogramas en el directorio\n",
    "    frame_files = sorted(os.listdir(frames_dir), key=lambda x: int(x.split('.')[0]))\n",
    "    \n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(frames_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        \n",
    "        if frame is not None:\n",
    "            frame_number = int(frame_file.split('.')[0])\n",
    "            preprocessed_frame = preprocess_frame(frame)\n",
    "            class_name, confidence = detect_objects_in_frame(preprocessed_frame)\n",
    "            \n",
    "            results.append({\n",
    "                'frame_number': frame_number,\n",
    "                'class_name': class_name,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    # Convertir los resultados a un DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Directorio que contiene los fotogramas del vídeo\n",
    "    frames_directory = r\"C:\\Users\\Usuario\\Desktop\\GitHub\\Grupo-Atrium\\Deep Learning\\frames\"\n",
    "    \n",
    "    # Procesar los fotogramas y obtener el DataFrame\n",
    "    results_df = process_video_frames(frames_directory)\n",
    "    \n",
    "    # Guardar los resultados en un archivo CSV\n",
    "    results_df.to_csv('resultados_deteccion_pytorch.csv', index=False)\n",
    "    \n",
    "    print(\"Detección de objetos completada. Resultados guardados en 'resultados_deteccion_pytorch.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
